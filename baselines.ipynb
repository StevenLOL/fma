{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# FMA: A Dataset For Music Analysis\n",
    "\n",
    "Kirell Benzi, MichaÃ«l Defferrard, Pierre Vandergheynst, Xavier Bresson, EPFL LTS2.\n",
    "\n",
    "## Baselines\n",
    "\n",
    "We explore three types of baselines:\n",
    "1. simple algorithms,\n",
    "2. state-of-the-art in genre recognition,\n",
    "3. deep Learning approaches,\n",
    "\n",
    "using different input features:\n",
    "1. raw audio,\n",
    "2. echonest features,\n",
    "3. audio features from librosa or [kapre](https://github.com/keunwoochoi/kapre).\n",
    "\n",
    "We aim at showing that given sufficient data, DL approaches can outperfom all the others without domain-specific / expert knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import utils\n",
    "import keras\n",
    "from keras.layers import Activation, Dense, Conv1D, Conv2D, MaxPooling1D, Flatten, Reshape\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "import time\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder, LabelBinarizer, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "#from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "#from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = os.environ.get('DATA_DIR')\n",
    "df = pd.read_json(os.path.join(DATA_DIR, 'fma_small.json'))\n",
    "path = utils.build_path(df, DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1 Simple classifiers on Echonest features\n",
    "\n",
    "Todo:\n",
    "* Cross-validation for hyper-parameters.\n",
    "* Dimensionality reduction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.1 Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Select features.\n",
    "#features = utils.ECHONEST_AUDIO_FEATURES + utils.ECHONEST_SOCIAL_FEATURES\n",
    "features = utils.ECHONEST_AUDIO_FEATURES\n",
    "\n",
    "# Discard songs with NaN Echonest features.\n",
    "# TODO: fix dataset.\n",
    "keep = df[features].isnull().apply(lambda x: not x.any(), axis=1)\n",
    "df = pd.DataFrame(df[keep])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def pre_process(df, features, multi_label=False):\n",
    "    if not multi_label:\n",
    "        # Assign an integer value to each genre.\n",
    "        enc = LabelEncoder()\n",
    "        y = enc.fit_transform(df['top_genre'])\n",
    "    else:\n",
    "        # Create an indicator matrix.\n",
    "        enc = MultiLabelBinarizer()\n",
    "        y = enc.fit_transform(df['genres'])\n",
    "    print('Genres ({}): {}'.format(len(enc.classes_), enc.classes_))\n",
    "\n",
    "    X = df[features].as_matrix()\n",
    "    \n",
    "    # Split in training, validation and testing sets.\n",
    "    train = df['train'] == True\n",
    "    y_train = y[train]\n",
    "    y_test = y[~train]\n",
    "    X_train = X[train]\n",
    "    X_test = X[~train]\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=.2, random_state=42)\n",
    "    print('{} training examples, {} validation examples, {} testing examples'.format(y_train.shape[0], y_val.shape[0], y_test.shape[0]))\n",
    "    print('{} features'.format(X_train.shape[1]))\n",
    "    \n",
    "    # Standardize features by removing the mean and scaling to unit variance.\n",
    "    scaler = StandardScaler(copy=False)\n",
    "    scaler.fit_transform(X_train)\n",
    "    scaler.transform(X_val)\n",
    "    scaler.transform(X_test)\n",
    "    \n",
    "    return y_train, y_val, y_test, X_train, X_val, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.2 Single genre\n",
    "\n",
    "Maximum observed is around 38%, both on `fma_small` and `fma_medium`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genres (10): ['Electronic' 'Folk' 'Hip-Hop' 'Indie-Rock' 'Jazz' 'Old-Time / Historic'\n",
      " 'Pop' 'Psych-Rock' 'Punk' 'Rock']\n",
      "2524 training examples, 631 validation examples, 788 testing examples\n",
      "8 features\n",
      "33.25% 0.04s LogisticRegression\n",
      "31.98% 0.07s KNeighborsClassifier\n",
      "36.04% 0.36s SVC\n",
      "33.50% 0.19s SVC\n",
      "33.25% 0.83s LinearSVC\n",
      "34.14% 0.01s DecisionTreeClassifier\n",
      "34.52% 0.02s RandomForestClassifier\n",
      "31.09% 0.04s AdaBoostClassifier\n",
      "36.29% 2.80s MLPClassifier\n",
      "31.47% 0.03s GaussianNB\n",
      "31.22% 0.05s QuadraticDiscriminantAnalysis\n"
     ]
    }
   ],
   "source": [
    "y_train, y_val, y_test, X_train, X_val, X_test = pre_process(df, features)\n",
    "\n",
    "classifiers = [\n",
    "    LogisticRegression(),\n",
    "    KNeighborsClassifier(n_neighbors=200),\n",
    "    SVC(),\n",
    "    SVC(kernel=\"linear\"),\n",
    "    LinearSVC(),\n",
    "    #GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    AdaBoostClassifier(n_estimators=10),\n",
    "    MLPClassifier(max_iter=400),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "]\n",
    "\n",
    "for clf in classifiers:\n",
    "    t = time.process_time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    print('{:.2f}% {:.2f}s {}'.format(score*100, time.process_time()-t, type(clf).__name__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.3 Multiple genres\n",
    "\n",
    "Maximum observed is around 11% for `fma_small` and 7.6% for `fma_medium`.\n",
    "\n",
    "TODO:\n",
    "* Eliminate rare genres. On small only the 10 selected genres are meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genres (108): ['20th Century Classical' 'African' 'Afrobeat' 'Alternative Hip-Hop'\n",
      " 'Americana' 'Asia-Far East' 'Balkan' 'Big Band/Swing' 'Bigbeat'\n",
      " 'Bluegrass' 'Bollywood' 'Brazilian' 'Breakbeat' 'Breakcore - Hard'\n",
      " 'British Folk' 'Chamber Music' 'Chill-out' 'Chip Music' 'Chiptune'\n",
      " 'Classical' 'Composed Music' 'Country' 'Country & Western' 'Cumbia'\n",
      " 'Dance' 'Disco' 'Downtempo' 'Drone' 'Dubstep' 'Easy Listening'\n",
      " 'Easy Listening: Vocal' 'Electro-Punk' 'Electroacoustic' 'Electronic'\n",
      " 'Europe' 'Flamenco' 'Folk' 'Freak-Folk' 'Free-Folk' 'Free-Jazz' 'French'\n",
      " 'Funk' 'Gospel' 'Goth' 'Hardcore' 'Hip-Hop' 'Hip-Hop Beats' 'Holiday'\n",
      " 'House' 'IDM' 'Improv' 'Indian' 'Indie-Rock' 'Industrial' 'Instrumental'\n",
      " 'Interview' 'Jazz' 'Jazz: Out' 'Jazz: Vocal' 'Klezmer' 'Krautrock' 'Latin'\n",
      " 'Latin America' 'Loud-Rock' 'Lounge' 'Metal' 'Middle East'\n",
      " 'Minimal Electronic' 'Minimalism' 'Modern Jazz' 'Musique Concrete'\n",
      " 'New Age' 'New Wave' 'No Wave' 'Nu-Jazz' 'Old-Time / Historic' 'Opera'\n",
      " 'Polka' 'Pop' 'Post-Punk' 'Post-Rock' 'Power-Pop' 'Progressive'\n",
      " 'Psych-Folk' 'Psych-Rock' 'Punk' 'Radio Theater' 'Rap'\n",
      " 'Reggae - Dancehall' 'Reggae - Dub' 'Rock' 'Rock Opera' 'Rockabilly'\n",
      " 'Romany (Gypsy)' 'Shoegaze' 'Skweee' 'Sludge' 'Soul-RnB' 'Sound Art'\n",
      " 'Soundtrack' 'Space-Rock' 'Spanish' 'Surf' 'Synth Pop' 'Techno' 'Thrash'\n",
      " 'Trip-Hop' 'Wonky']\n",
      "2524 training examples, 631 validation examples, 788 testing examples\n",
      "8 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.pyenv/versions/3.6.0/envs/fma/lib/python3.6/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 22 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/ubuntu/.pyenv/versions/3.6.0/envs/fma/lib/python3.6/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 55 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/ubuntu/.pyenv/versions/3.6.0/envs/fma/lib/python3.6/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 69 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/ubuntu/.pyenv/versions/3.6.0/envs/fma/lib/python3.6/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 74 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/ubuntu/.pyenv/versions/3.6.0/envs/fma/lib/python3.6/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 76 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/ubuntu/.pyenv/versions/3.6.0/envs/fma/lib/python3.6/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 88 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/ubuntu/.pyenv/versions/3.6.0/envs/fma/lib/python3.6/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 91 is present in all training examples.\n",
      "  str(classes[c]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.15% 0.47s OneVsRestClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.pyenv/versions/3.6.0/envs/fma/lib/python3.6/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 22 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/ubuntu/.pyenv/versions/3.6.0/envs/fma/lib/python3.6/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 55 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/ubuntu/.pyenv/versions/3.6.0/envs/fma/lib/python3.6/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 69 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/ubuntu/.pyenv/versions/3.6.0/envs/fma/lib/python3.6/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 74 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/ubuntu/.pyenv/versions/3.6.0/envs/fma/lib/python3.6/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 76 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/ubuntu/.pyenv/versions/3.6.0/envs/fma/lib/python3.6/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 88 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/ubuntu/.pyenv/versions/3.6.0/envs/fma/lib/python3.6/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 91 is present in all training examples.\n",
      "  str(classes[c]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.42% 2.08s OneVsRestClassifier\n"
     ]
    }
   ],
   "source": [
    "y_train, y_val, y_test, X_train, X_val, X_test = pre_process(df, features, multi_label=True)\n",
    "\n",
    "classifiers = [\n",
    "    #LogisticRegression(),\n",
    "    OneVsRestClassifier(LogisticRegression()),\n",
    "    OneVsRestClassifier(SVC()),\n",
    "]\n",
    "\n",
    "for clf in classifiers:\n",
    "    t = time.process_time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    print('{:.2f}% {:.2f}s {}'.format(score*100, time.process_time()-t, type(clf).__name__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2 Deep learning on raw audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: fix dataset.\n",
    "df['top_genre'] = df['top_genre'].apply(lambda genre: 'Old-Time' if genre == 'Old-Time / Historic' else genre)\n",
    "\n",
    "# TODO: fix dataset.\n",
    "# Clips with less than 1321967 samples because lower sampling rate, or mono.\n",
    "BAD_CLIPS = [16402, 16425, 16406, 16431, 33709, 16352, 16404, 33708, 31375, 33702, 22590, 22591, 16039,\n",
    "             12856, 33716, 16426, 16422, 16421, 16405, 16427, 16401, 16038, 16424, 16429, 16351, 16428,\n",
    "             16039, 33716, 33702, 31375, 16422, 16352, 22591, 16426, 16429, 16038, 16401, 12856, 16404,\n",
    "             16402, 16428, 16425, 16405, 33708, 16424, 33709, 16427, 16431, 22590, 16351, 33714, 16421, 16406]\n",
    "BAD_CLIPS.extend([11665, 12899, 12916, 12917, 16353, 16398, 16400, 16423, 16430, 18689, 18691])\n",
    "\n",
    "df = df.drop(BAD_CLIPS)\n",
    "path = utils.build_path(df, DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "labels_onehot = LabelBinarizer().fit_transform(df.top_genre)\n",
    "\n",
    "train = np.argwhere(df['train'] == True).flatten()\n",
    "test = np.argwhere(df['train'] == False).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Load audio samples in parallel using `multiprocessing` so as to maximize CPU usage when decoding MP3s and making some optional pre-processing. There are multiple ways to load a waveform from a compressed MP3:\n",
    "* librosa uses audioread in the backend which can use many native libraries, e.g. ffmpeg\n",
    "    * resampling is very slow\n",
    "    * does not work with multi-processing, for keras `fit_generator()`\n",
    "* pydub is a high-level interface for audio modification, uses ffmpeg to load\n",
    "    * store a temporary `.wav`\n",
    "* directly pipe ffmpeg output\n",
    "    * fastest method\n",
    "* [pyAV](https://github.com/mikeboers/PyAV) may be a fastest alternative by linking to ffmpeg libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1321967)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just be sure that everything is fine. Multiprocessing is tricky to debug.\n",
    "utils.FfmpegLoader().load(path(0))\n",
    "SampleLoader = utils.build_sample_loader(path, labels_onehot, utils.FfmpegLoader())\n",
    "SampleLoader(train, batch_size=2).__next__()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Keras parameters.\n",
    "NB_WORKER = len(os.sched_getaffinity(0))  # number of usables CPUs\n",
    "params = {'pickle_safe': True, 'nb_worker': NB_WORKER, 'max_q_size': 10}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.1 Fully connected neural network\n",
    "\n",
    "* Two layers with 10 hiddens is no better than random, ~11%.\n",
    "\n",
    "Optimize data loading to be CPU / GPU bound, not IO bound. Larger batches means reduced training time, so increase batch time until memory exhaustion. Number of workers and queue size have no influence on speed.\n",
    "\n",
    "CPU\n",
    "* batch 4, worker 8, queue 1, 600s\n",
    "* batch 20, worker 24, queue 5, 190s\n",
    "* batch 20, worker 12, queue 10, 185s\n",
    "* batch 40, worker 12, queue 10, 135s\n",
    "* batch 64, worker 12, queue 10, 110s\n",
    "* batch 128, worker 12, queue 10, 100s\n",
    "\n",
    "GPU Tesla K40c\n",
    "* batch 4, worker 12, queue 10, 250s\n",
    "* batch 16, worker 12, queue 10, 100s\n",
    "* batch 32, worker 12, queue 10, 90s\n",
    "* batch 64, worker 12, queue 10, 70s\n",
    "* batch 96-128 --> memory error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionality: (59953,)\n",
      "Epoch 1/2\n",
      "3128/3128 [==============================] - 57s - loss: 14.5413 - acc: 0.0978    \n",
      "Epoch 2/2\n",
      "3128/3128 [==============================] - 47s - loss: 14.4846 - acc: 0.1013    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[14.458574412872432, 0.10296010296969187]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = utils.FfmpegLoader(sampling_rate=2000)\n",
    "SampleLoader = utils.build_sample_loader(path, labels_onehot, loader)\n",
    "print('Dimensionality: {}'.format(loader.shape))\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(Dense(output_dim=1000, input_shape=loader.shape))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(output_dim=100))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(output_dim=labels_onehot.shape[1]))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(lr=0.1, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit_generator(SampleLoader(train, batch_size=64), train.size, nb_epoch=2, **params)\n",
    "loss = model.evaluate_generator(SampleLoader(test, batch_size=64), test.size, **params)\n",
    "#Y = model.predict_generator(SampleLoader(test, batch_size=64), test.size, **params);\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.2 Convolutional neural network\n",
    "\n",
    "* Architecture from [End-to-end learning for music audio](http://www.mirlab.org/conference_papers/International_Conference/ICASSP%202014/papers/p7014-dieleman.pdf) by Sander Dieleman, Benjamin Schrauwen.\n",
    "* Missing: track segmentation and majority voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 479625, 1)\n",
      "(None, 936, 128)\n",
      "(None, 929, 32)\n",
      "(None, 225, 32)\n",
      "(None, 56, 32)\n",
      "(None, 1792)\n",
      "(None, 100)\n",
      "(None, 10)\n",
      "Epoch 1/2\n",
      "3128/3128 [==============================] - 49s - loss: 14.4991 - acc: 0.1004    \n",
      "Epoch 2/2\n",
      "3130/3128 [==============================] - 49s - loss: 14.4908 - acc: 0.1010    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.pyenv/versions/3.6.0/envs/fma/lib/python3.6/site-packages/keras/engine/training.py:1573: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.\n",
      "  warnings.warn('Epoch comprised more than '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[14.479318942802752, 0.10167310336074928]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = utils.FfmpegLoader(sampling_rate=16000)\n",
    "#loader = utils.LibrosaLoader(sampling_rate=16000)\n",
    "SampleLoader = utils.build_sample_loader(path, labels_onehot, loader)\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(Reshape((-1, 1), input_shape=loader.shape))\n",
    "print(model.output_shape)\n",
    "\n",
    "model.add(Conv1D(128, 512, subsample_length=512))\n",
    "print(model.output_shape)\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(Conv1D(32, 8))\n",
    "print(model.output_shape)\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling1D(4))\n",
    "\n",
    "model.add(Conv1D(32, 8))\n",
    "print(model.output_shape)\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling1D(4))\n",
    "\n",
    "print(model.output_shape)\n",
    "#model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "print(model.output_shape)\n",
    "model.add(Dense(100))\n",
    "model.add(Activation(\"relu\"))\n",
    "print(model.output_shape)\n",
    "model.add(Dense(labels_onehot.shape[1]))\n",
    "model.add(Activation(\"softmax\"))\n",
    "print(model.output_shape)\n",
    "\n",
    "optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "#optimizer = keras.optimizers.Adam()#lr=1e-5)#, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit_generator(SampleLoader(train, batch_size=10), train.size, nb_epoch=2, **params)\n",
    "loss = model.evaluate_generator(SampleLoader(test, batch_size=10), test.size, **params)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.3 Recurrent neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3 Deep learning on extracted audio features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.1 ConvNet on MFCC\n",
    "\n",
    "* Architecture from [Automatic Musical Pattern Feature Extraction Using Convolutional Neural Network](http://www.iaeng.org/publication/IMECS2010/IMECS2010_pp546-550.pdf) by Tom LH. Li, Antoni B. Chan and Andy HW. Chun\n",
    "* Missing: track segmentation and majority voting.\n",
    "* Best seen: 17.6%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2582,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MfccLoader(utils.Loader):\n",
    "    raw_loader = utils.FfmpegLoader(sampling_rate=22050)\n",
    "    #shape = (13, 190)  # For segmented tracks.\n",
    "    shape = (13, 2582)\n",
    "    def load(self, filename):\n",
    "        import librosa\n",
    "        x = self.raw_loader.load(filename)\n",
    "        # Each MFCC frame spans 23ms on the audio signal with 50% overlap with the adjacent frames.\n",
    "        mfcc = librosa.feature.mfcc(x, sr=22050, n_mfcc=13, n_fft=512, hop_length=256)\n",
    "        return mfcc\n",
    "\n",
    "loader = MfccLoader()\n",
    "SampleLoader = utils.build_sample_loader(path, labels_onehot, loader)\n",
    "loader.load(path(0))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 13, 2582, 1)\n",
      "(None, 1, 644, 3)\n",
      "(None, 1, 159, 15)\n",
      "(None, 1, 38, 65)\n",
      "(None, 2470)\n",
      "(None, 10)\n",
      "Epoch 1/2\n",
      "3128/3128 [==============================] - 107s - loss: 2.5369 - acc: 0.0857   \n",
      "Epoch 2/2\n",
      "3128/3128 [==============================] - 105s - loss: 2.3029 - acc: 0.0886   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.3026296003137929, 0.10167310167310167]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(Reshape((*loader.shape, 1),  input_shape=loader.shape))\n",
    "print(model.output_shape)\n",
    "\n",
    "model.add(Conv2D(3, 13, 10, subsample=(1, 4)))\n",
    "model.add(Activation(\"relu\"))\n",
    "print(model.output_shape)\n",
    "\n",
    "model.add(Conv2D(15, 1, 10, subsample=(1, 4)))\n",
    "model.add(Activation(\"relu\"))\n",
    "print(model.output_shape)\n",
    "\n",
    "model.add(Conv2D(65, 1, 10, subsample=(1, 4)))\n",
    "model.add(Activation(\"relu\"))\n",
    "print(model.output_shape)\n",
    "\n",
    "model.add(Flatten())\n",
    "print(model.output_shape)\n",
    "model.add(Dense(labels_onehot.shape[1]))\n",
    "model.add(Activation(\"softmax\"))\n",
    "print(model.output_shape)\n",
    "\n",
    "optimizer = keras.optimizers.SGD(1e-3)#lr=0.01, momentum=0.9, nesterov=True)\n",
    "#optimizer = keras.optimizers.Adam()#lr=1e-5)#\n",
    "model.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit_generator(SampleLoader(train, batch_size=16), train.size, nb_epoch=2, **params)\n",
    "loss = model.evaluate_generator(SampleLoader(test, batch_size=16), test.size, **params)\n",
    "#Y = model.predict_generator(loader, test.size, pickle_safe=True, nb_worker=NB_WORKER, max_q_size=5)\n",
    "\n",
    "loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
